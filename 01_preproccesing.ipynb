{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from seaborn) (1.5.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from seaborn) (3.9.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bertopic in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (0.16.2)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (0.8.36)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (1.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (1.1.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (5.11.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (3.0.0)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (0.5.6)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from bertopic) (4.66.2)\n",
      "Requirement already satisfied: cython<3,>=0.27 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.9.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (8.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.23.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.40.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: Pillow in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (9.2.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.9.0)\n",
      "Requirement already satisfied: requests in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (23.2)\n",
      "Requirement already satisfied: filelock in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.42.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: networkx in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (2024.4.28)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osakiyutaka/.pyenv/versions/3.10.4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install bertopic\n",
    "from zhconv import convert\n",
    "import pandas as pd\n",
    "import re\n",
    "import esupar\n",
    "from supar.utils.transform import CoNLLSentence\n",
    "from itertools import combinations\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import combinations\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To make raw data tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make tidy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tidy_data(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the given DataFrame by splitting the 'content' column into paragraphs,\n",
    "    removing certain characters, exploding the DataFrame, adding a new column to indicate\n",
    "    if a paragraph contains the character '仁', and removing any unnamed columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df['content'] = df['content'].apply(lambda x: x.split(\"\\n\\n\"))\n",
    "        df['content'] = df['content'].apply(lambda x: [re.sub(r'[「」]', '\"', i) for i in x])\n",
    "        df = df.explode('content')\n",
    "        df['contain_ren'] = df['content'].apply(lambda x: 1 if '仁' in x else 0)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    except:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_tidy_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/modified_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To add time attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bookname = list(df[\"book\"].unique())\n",
    "list_book_time = [\n",
    "    ['analects',-480, -350],\n",
    "    ['mengzi',  -340, -250],\n",
    "    ['liji',    -475, -221],\n",
    "    ['xunzi',   -475, -221],\n",
    "    ['xiao-jing',-475, -221],\n",
    "    ['shuo-yuan',-206, 9],\n",
    "    ['chun-qiu-fan-lu',-206, 9],\n",
    "    ['han-shi-wai-zhuan',-180, -120],\n",
    "    ['da-dai-li-ji',100, 200],\n",
    "    ['bai-hu-tong',79, 92],\n",
    "    ['xin-shu', -206, 9],\n",
    "    ['xin-xu',-206, 9],\n",
    "    ['yangzi-fayan',-33, 18],\n",
    "    ['zhong-lun',25, 220],\n",
    "    ['kongzi-jiayu',-206, 220],\n",
    "    ['qian-fu-lun',102, 167],\n",
    "    ['lunheng', 80, 80],\n",
    "    ['taixuanjing',-33, 18],\n",
    "    ['fengsutongyi',190, 200],\n",
    "    ['kongcongzi',25, 265],\n",
    "    ['shenjian',196, 220],\n",
    "    ['zhong-jing',100,166],\n",
    "    ['su-shu',-250,-186],\n",
    "    ['xinyu',-196, -196],\n",
    "    ['duduan',167, 258],\n",
    "    ['caizhong-langji', 152, 192]\n",
    "]\n",
    "def map_year(year):\n",
    "    return int(year/2 + 2000)\n",
    "def set_time_attributes(df):\n",
    "    \"\"\"\n",
    "    This function sets the time attributes \"Start\", \"End\", and \"era\" for each book in the input dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe. It should contain a column named \"book\".\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input dataframe with three new columns: \"Start\", \"End\", and \"era\". Each row represents a book.\n",
    "    - \"Start\" is the start year of the book.\n",
    "    - \"End\" is the end year of the book.\n",
    "    - \"era\" is the average of the start year and the end year.\n",
    "    \"\"\"\n",
    "    df[\"Start\"] = None  # Initialize the \"Start\" column\n",
    "    df[\"End\"] = None  # Initialize the \"End\" column\n",
    "    df[\"average_year\"] = None\n",
    "    df['mapped_year'] = None\n",
    "    num = list_bookname.index(df.iloc[0][\"book\"])  # Get the index of the first book in the list of book names\n",
    "    for i in range(df.shape[0]):  # For each row in the dataframe\n",
    "        num = list_bookname.index(df.iloc[i][\"book\"])  # Get the index of the book in the list of book names\n",
    "        df.loc[i, 'Start'] = list_book_time[num][1]  # Set the \"Start\" attribute\n",
    "        df.loc[i, 'End'] = list_book_time[num][2]  # Set the \"End\" attribute\n",
    "    for i in range(df.shape[0]):  # For each row in the dataframe\n",
    "        df.loc[i, 'average_year'] = int((df.loc[i, 'Start']+df.loc[i, 'End'])/2)\n",
    "    df['mapped_year'] = df['average_year'].apply(map_year)\n",
    "    return df  # Return the dataframe with the new attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>content</th>\n",
       "      <th>contain_ren</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>average_year</th>\n",
       "      <th>mapped_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：\"學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？\"</td>\n",
       "      <td>0</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>有子曰：\"其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道...</td>\n",
       "      <td>1</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：\"巧言令色，鮮矣仁！\"</td>\n",
       "      <td>1</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>曾子曰：\"吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？\"</td>\n",
       "      <td>0</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analects</td>\n",
       "      <td>學而</td>\n",
       "      <td>子曰：\"道千乘之國：敬事而信，節用而愛人，使民以時。\"</td>\n",
       "      <td>0</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter                                            content  \\\n",
       "0  analects      學而          子曰：\"學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？\"   \n",
       "1  analects      學而  有子曰：\"其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道...   \n",
       "2  analects      學而                                     子曰：\"巧言令色，鮮矣仁！\"   \n",
       "3  analects      學而                曾子曰：\"吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？\"   \n",
       "4  analects      學而                        子曰：\"道千乘之國：敬事而信，節用而愛人，使民以時。\"   \n",
       "\n",
       "   contain_ren Start   End average_year  mapped_year  \n",
       "0            0  -480  -350         -415         1792  \n",
       "1            1  -480  -350         -415         1792  \n",
       "2            1  -480  -350         -415         1792  \n",
       "3            0  -480  -350         -415         1792  \n",
       "4            0  -480  -350         -415         1792  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = set_time_attributes(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the DataFrame is 12604\n",
      "The number of sentences that contain the character \"仁\" is 921\n"
     ]
    }
   ],
   "source": [
    "print(f'The length of the DataFrame is {len(df)}', \n",
    "      f'The number of sentences that contain the character \"仁\" is {len(df[df[\"contain_ren\"] == 1])}', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chaprers in three periods: 697\n",
      "The number of chapters in period 1: 139\n",
      "The number of chapters in period 2: 293\n",
      "The number of chapters in period 3: 265\n"
     ]
    }
   ],
   "source": [
    "#to see the number of chapter in three periods: ~202bc, 202bc-25ac, 25ac~\n",
    "df_period1 = df[df['average_year'] <= -202]\n",
    "df_period1_filtered = df_period1[['book', 'chapter']].drop_duplicates()\n",
    "df_period2 = df[(df['average_year'] > -202) & (df['average_year'] <= 25)]\n",
    "df_period2_filtered = df_period2[['book', 'chapter']].drop_duplicates()\n",
    "df_period3 = df[df['average_year'] > 25]\n",
    "df_period3_filtered = df_period3[['book', 'chapter']].drop_duplicates()\n",
    "print(\"The number of chaprers in three periods:\", len(df_period1_filtered)+len(df_period2_filtered)+len(df_period3_filtered))\n",
    "print('The number of chapters in period 1:', len(df_period1_filtered))\n",
    "print('The number of chapters in period 2:', len(df_period2_filtered))\n",
    "print('The number of chapters in period 3:', len(df_period3_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do word segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load tidy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/modified_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at KoichiYasuoka/roberta-classical-chinese-base-upos and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def word_segmentation(text):\n",
    "    \"\"\"\n",
    "    This function segments the input text into words and returns the segmented words and their parts of speech.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be segmented.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple of two lists. The first list contains the segmented words. The second list contains the parts of speech of the segmented words.\n",
    "    \"\"\"\n",
    "    conllu_data = nlp(text)\n",
    "    word_segmented = conllu_data.values[1]\n",
    "    word_segmented_pos = conllu_data.values[3]\n",
    "    return word_segmented, word_segmented_pos\n",
    "\n",
    "nlp = esupar.load(\"KoichiYasuoka/roberta-classical-chinese-base-upos\")\n",
    "\n",
    "def co_occurence_preparation(df, bookindex):\n",
    "    \"\"\"\n",
    "    This function prepares a co-occurrence dataframe for a specific book in the input dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe. It should contain the following columns: \"book\", \"chapter\", \"sentences\", \"Start\", and \"End\".\n",
    "    bookindex (int): The index of the book to be processed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe with the following columns: \"Item1\", \"Item2\", \"book\", \"chapter\", \"sentences_index\", \"contain_ren\", \"Start\", and \"End\". Each row represents a pair of co-occurring words in a sentence of the specified book.\n",
    "    \"\"\"\n",
    "    df_cooccurrence = pd.DataFrame(columns=['Item1', 'Item2', 'book', 'chapter', 'sentences_index', 'contain_ren', 'Start', 'End'])\n",
    "    for index, row in df[df[\"book\"] == df[\"book\"].unique()[bookindex]].iterrows():\n",
    "        print(df[\"book\"][bookindex])\n",
    "        try:\n",
    "            word_segmented, word_segmented_pos = word_segmentation(row[\"sentences\"])\n",
    "            pairs = list(combinations(word_segmented, 2))\n",
    "            tmp_df = pd.DataFrame(pairs, columns=['Item1', 'Item2'])\n",
    "            tmp_df['book'] = row['book']\n",
    "            tmp_df['chapter'] = row['chapter']\n",
    "            tmp_df['sentences_index'] = index\n",
    "            tmp_df['contain_ren'] = tmp_df.apply(lambda x: '仁' in x['Item1'] or '仁' in x['Item2'], axis=1)\n",
    "            tmp_df['Start'] = row['Start']\n",
    "            tmp_df['End'] = row['End']\n",
    "            df_cooccurrence = pd.concat([df_cooccurrence, tmp_df], ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "    return df_cooccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cooccurence_analect = co_occurence_preparation(df, len(df[\"book\"].unique()))\n",
    "df_ren = df_cooccurence_analect[df_cooccurence_analect[\"contain_ren\"]==True]\n",
    "df_ren = df_ren.reset_index(drop=True)\n",
    "df_ren.to_csv('data/data_ren.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at KoichiYasuoka/roberta-classical-chinese-base-upos and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp = esupar.load(\"KoichiYasuoka/roberta-classical-chinese-base-upos\")\n",
    "\n",
    "def word_segmentation(text):\n",
    "    \"\"\"\n",
    "    This function performs word segmentation on the input text using a pre-trained model.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be segmented. If the input is not a string or is empty, the function returns an empty string.\n",
    "\n",
    "    Returns:\n",
    "    str: The segmented text, with each word separated by a space. If the input is not a string or is empty, the function returns an empty string.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    conllu_data = nlp(text)\n",
    "    word_segmented = conllu_data.values[1]\n",
    "    word_segmented = ' '.join(word_segmented)\n",
    "    return word_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the content to Simplified Chinese\n",
    "data['chapter'] = data['chapter'].apply(lambda x: convert(x, 'zh-cn'))\n",
    "data['content'] = data['content'].apply(lambda x: convert(x, 'zh-cn'))\n",
    "\n",
    "# Create 'book', 'chapter', 'sentences' structure\n",
    "data = data[['book', 'chapter', 'content']]\n",
    "\n",
    "data = set_time_attributes(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>content</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>average_year</th>\n",
       "      <th>mapped_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”\\n\\n有...</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analects</td>\n",
       "      <td>为政</td>\n",
       "      <td>子曰：“为政以德，譬如北辰，居其所而众星共之。”\\n\\n子曰：“诗三百，一言以蔽之，曰‘思无...</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analects</td>\n",
       "      <td>八佾</td>\n",
       "      <td>孔子谓季氏：“八佾舞于庭，是可忍也，孰不可忍也？”\\n\\n三家者以雍彻。子曰：“‘相维辟公，...</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analects</td>\n",
       "      <td>里仁</td>\n",
       "      <td>子曰：“里仁为美。择不处仁，焉得知？”\\n\\n子曰：“不仁者不可以久处约，不可以长处乐。仁者...</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analects</td>\n",
       "      <td>公冶长</td>\n",
       "      <td>子谓公冶长，“可妻也。虽在缧绁之中，非其罪也”。以其子妻之。\\n\\n子谓南容，“邦有道，不废...</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter                                            content Start  \\\n",
       "0  analects      学而  子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？”\\n\\n有...  -480   \n",
       "1  analects      为政  子曰：“为政以德，譬如北辰，居其所而众星共之。”\\n\\n子曰：“诗三百，一言以蔽之，曰‘思无...  -480   \n",
       "2  analects      八佾  孔子谓季氏：“八佾舞于庭，是可忍也，孰不可忍也？”\\n\\n三家者以雍彻。子曰：“‘相维辟公，...  -480   \n",
       "3  analects      里仁  子曰：“里仁为美。择不处仁，焉得知？”\\n\\n子曰：“不仁者不可以久处约，不可以长处乐。仁者...  -480   \n",
       "4  analects     公冶长  子谓公冶长，“可妻也。虽在缧绁之中，非其罪也”。以其子妻之。\\n\\n子谓南容，“邦有道，不废...  -480   \n",
       "\n",
       "    End average_year  mapped_year  \n",
       "0  -350         -415         1792  \n",
       "1  -350         -415         1792  \n",
       "2  -350         -415         1792  \n",
       "3  -350         -415         1792  \n",
       "4  -350         -415         1792  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting all data into words took much time, so thic code is an example using one chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 16:06:04 INFO Loading the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 16:06:04 INFO \n",
      "Dataset(n_sentences=1, n_batches=1, n_buckets=1)\n",
      "2024-06-04 16:06:04 INFO Making predictions on the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 16:06:25 INFO 0:00:20.238353s elapsed, 0.05 Sents/s\n"
     ]
    }
   ],
   "source": [
    "# Apply the 'word_segmentation' function to the 'content' column, write the results to a txt file, and save after each step\n",
    "with open('data/word_segmentation_tmp.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in data['content']:\n",
    "        seg_sentence = word_segmentation(sentence)\n",
    "        f.write(seg_sentence + '\\n')\n",
    "        f.flush()  # Flush the file content to disk\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pre-segmented txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output.txt file, and load it into the data dataframe, as a new column called 'segmented'\n",
    "if os.path.exists('data/word_segmentation.txt'):\n",
    "    with open('data/word_segmentation.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    data['segmented'] = lines\n",
    "data.drop(columns=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = set_time_attributes(df)\n",
    "# Add a new column 'contain_ren' to check if the sentence contains the character '仁'\n",
    "data['contain_ren'] = data['segmented'].apply(lambda x: 1 if '仁' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>average_year</th>\n",
       "      <th>mapped_year</th>\n",
       "      <th>segmented</th>\n",
       "      <th>contain_ren</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analects</td>\n",
       "      <td>学而</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "      <td>子 曰 学 而 时 习 之 不 亦 说 乎 有 朋 自 远 方 来 不 亦 乐 乎 人 不 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analects</td>\n",
       "      <td>为政</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "      <td>子 曰 为 政 以 德 譬 如 北 辰 居 其 所 而 众 星 共 之 子 曰 诗 三百 一...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analects</td>\n",
       "      <td>八佾</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "      <td>孔子 谓 季 氏 八 佾 舞 于 庭 是 可 忍 也 孰 不 可 忍 也 三 家 者 以 雍...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analects</td>\n",
       "      <td>里仁</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "      <td>子 曰 里 仁 为 美 择 不 处 仁 焉 得 知 子 曰 不 仁 者 不 可 以 久 处 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analects</td>\n",
       "      <td>公冶长</td>\n",
       "      <td>-480</td>\n",
       "      <td>-350</td>\n",
       "      <td>-415</td>\n",
       "      <td>1792</td>\n",
       "      <td>子 谓 公冶 长 可 妻 也 虽 在 缧 绁 之 中 非 其 罪 也 以 其 子 妻 之 子...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book chapter Start   End average_year  mapped_year  \\\n",
       "0  analects      学而  -480  -350         -415         1792   \n",
       "1  analects      为政  -480  -350         -415         1792   \n",
       "2  analects      八佾  -480  -350         -415         1792   \n",
       "3  analects      里仁  -480  -350         -415         1792   \n",
       "4  analects     公冶长  -480  -350         -415         1792   \n",
       "\n",
       "                                           segmented  contain_ren  \n",
       "0  子 曰 学 而 时 习 之 不 亦 说 乎 有 朋 自 远 方 来 不 亦 乐 乎 人 不 ...            1  \n",
       "1  子 曰 为 政 以 德 譬 如 北 辰 居 其 所 而 众 星 共 之 子 曰 诗 三百 一...            0  \n",
       "2  孔子 谓 季 氏 八 佾 舞 于 庭 是 可 忍 也 孰 不 可 忍 也 三 家 者 以 雍...            1  \n",
       "3  子 曰 里 仁 为 美 择 不 处 仁 焉 得 知 子 曰 不 仁 者 不 可 以 久 处 ...            1  \n",
       "4  子 谓 公冶 长 可 妻 也 虽 在 缧 绁 之 中 非 其 罪 也 以 其 子 妻 之 子...            1  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
